{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Name: Sydney Huppert\n",
    "#Topic: ChiPy Mentorship Fall 2017\n",
    "#Project: Tweet word frequency analysis\n",
    "\n",
    "#Part 1: Frequency of Trump's twitter words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os as os\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "#years_avail = ['2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017']\n",
    "years_avail=['2016']\n",
    "file_string = '/Users/sydne/Documents/Chipy Tweets/'\n",
    "files = {\"Trump\": os.path.join(file_string, 'condensed_2016.json'), 'Clinton': os.path.join(file_string, 'hrc2016.json')}\n",
    "t_tokenizer = TweetTokenizer(preserve_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected object or value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-9ca338fabcfb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0myear\u001b[0m \u001b[1;32min\u001b[0m \u001b[0myears_avail\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfile_string\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0myear\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.json'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mcondensed_tweets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[1;31m#isolate text column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mtweet_texts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcondensed_tweets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\sydne\\Anaconda3\\lib\\site-packages\\pandas\\io\\json\\json.py\u001b[0m in \u001b[0;36mread_json\u001b[1;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, lines)\u001b[0m\n\u001b[0;32m    352\u001b[0m         obj = FrameParser(json, orient, dtype, convert_axes, convert_dates,\n\u001b[0;32m    353\u001b[0m                           \u001b[0mkeep_default_dates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprecise_float\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 354\u001b[1;33m                           date_unit).parse()\n\u001b[0m\u001b[0;32m    355\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'series'\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\sydne\\Anaconda3\\lib\\site-packages\\pandas\\io\\json\\json.py\u001b[0m in \u001b[0;36mparse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    420\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 422\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parse_no_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    423\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\sydne\\Anaconda3\\lib\\site-packages\\pandas\\io\\json\\json.py\u001b[0m in \u001b[0;36m_parse_no_numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    637\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0morient\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"columns\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    638\u001b[0m             self.obj = DataFrame(\n\u001b[1;32m--> 639\u001b[1;33m                 loads(json, precise_float=self.precise_float), dtype=None)\n\u001b[0m\u001b[0;32m    640\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0morient\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"split\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m             decoded = dict((str(k), v)\n",
      "\u001b[1;31mValueError\u001b[0m: Expected object or value"
     ]
    }
   ],
   "source": [
    "#Loop through all of the files and store the frequencies\n",
    "#time to look for words\n",
    "hits = {}\n",
    "filler_words = set(stopwords.words('english'))\n",
    "#note for later: define punctuation set\n",
    "filler_words.add('.')\n",
    "filler_words.add(',')\n",
    "filler_words.add('//')\n",
    "filler_words.add(':')\n",
    "filler_words.add('https')\n",
    "\n",
    "total_tweets = 0\n",
    "\n",
    "for year in years_avail:\n",
    "    file = file_string + year + '.json'\n",
    "    condensed_tweets = pd.read_json(file)\n",
    "    #isolate text column\n",
    "    tweet_texts = condensed_tweets['text']\n",
    "    total_tweets += len(tweet_texts)\n",
    "    for tweet in tweet_texts:\n",
    "        tweet_tokens = t_tokenizer.tokenize(tweet)\n",
    "        print(tweet_tokens)\n",
    "        tweet = []\n",
    "        for w in tweet_tokens:\n",
    "            if w not in filler_words and w.find('//') and (w.isalnum() or w.startswith('@')):\n",
    "                if w in hits:\n",
    "                    hits[w] = hits[w] + 1\n",
    "                else:\n",
    "                    hits[w] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('great', 899),\n",
       " ('thank', 748),\n",
       " ('trump', 541),\n",
       " ('hillary', 494),\n",
       " ('@realdonaldtrump', 443),\n",
       " ('people', 406),\n",
       " ('rt', 400),\n",
       " ('america', 392),\n",
       " ('new', 320),\n",
       " ('clinton', 315),\n",
       " ('make', 310),\n",
       " ('get', 274),\n",
       " ('big', 270),\n",
       " ('today', 264),\n",
       " ('president', 250),\n",
       " ('crooked', 243),\n",
       " ('u', 225),\n",
       " ('vote', 216),\n",
       " ('join', 205),\n",
       " ('many', 200),\n",
       " ('media', 198),\n",
       " ('cruz', 197),\n",
       " ('time', 196),\n",
       " ('jobs', 195),\n",
       " ('would', 193),\n",
       " ('country', 192),\n",
       " ('never', 183),\n",
       " ('going', 180),\n",
       " ('news', 180),\n",
       " ('one', 174),\n",
       " ('like', 173),\n",
       " ('bad', 164),\n",
       " ('american', 164),\n",
       " ('back', 162),\n",
       " ('tonight', 157),\n",
       " ('win', 150),\n",
       " ('much', 143),\n",
       " ('watch', 141),\n",
       " ('last', 138),\n",
       " ('said', 137),\n",
       " ('tomorrow', 133),\n",
       " ('us', 133),\n",
       " ('ted', 130),\n",
       " ('@foxnews', 129),\n",
       " ('obama', 128),\n",
       " ('poll', 126),\n",
       " ('good', 125),\n",
       " ('must', 123),\n",
       " ('even', 122),\n",
       " ('day', 122),\n",
       " ('want', 121),\n",
       " ('election', 121),\n",
       " ('campaign', 121),\n",
       " ('enjoy', 120),\n",
       " ('support', 120),\n",
       " ('@foxandfriends', 117),\n",
       " ('night', 116),\n",
       " ('fake', 116),\n",
       " ('see', 114),\n",
       " ('state', 113),\n",
       " ('w', 113),\n",
       " ('years', 111),\n",
       " ('republican', 111),\n",
       " ('@cnn', 110),\n",
       " ('go', 110),\n",
       " ('job', 109),\n",
       " ('way', 108),\n",
       " ('florida', 107),\n",
       " ('ohio', 102),\n",
       " ('speech', 100),\n",
       " ('donald', 99),\n",
       " ('states', 99),\n",
       " ('made', 99),\n",
       " ('look', 97),\n",
       " ('wow', 96),\n",
       " ('amazing', 96),\n",
       " ('first', 95),\n",
       " ('iowa', 93),\n",
       " ('love', 92),\n",
       " ('together', 91),\n",
       " ('rally', 91),\n",
       " ('north', 91),\n",
       " ('totally', 90),\n",
       " ('hard', 90),\n",
       " ('honor', 89),\n",
       " ('soon', 89),\n",
       " ('carolina', 87),\n",
       " ('united', 86),\n",
       " ('ever', 86),\n",
       " ('obamacare', 86),\n",
       " ('morning', 85),\n",
       " ('interviewed', 85),\n",
       " ('women', 85),\n",
       " ('national', 85),\n",
       " ('really', 84),\n",
       " ('debate', 84),\n",
       " ('democrats', 84),\n",
       " ('failing', 84),\n",
       " ('meeting', 82),\n",
       " ('show', 82)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import operator\n",
    "words_ordered = sorted(hits.items(), key=operator.itemgetter(1), reverse=True)\n",
    "list(words_ordered)[:100]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tweets: 5946\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total tweets: {total_tweets}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
